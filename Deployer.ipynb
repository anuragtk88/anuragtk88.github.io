{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ________________Musk Notifier__________________\n",
    "\n",
    "*I chose to work on this project since it involves aspects such as web scraping, data collection, cleaning, augmentation, training, and deployment.*\n",
    "\n",
    "## Introduction and problem statement\n",
    "Wiki: *\"Elon Reeve Musk FRS (/ˈiːlɒn/; born June 28, 1971) is an engineer, industrial designer, and technology entrepreneur. In December 2016, he was ranked 21st on the Forbes list of The World's Most Powerful People, and was ranked joint-first on the Forbes list of the Most Innovative Leaders of 2019.\"*\n",
    "\n",
    "Videos of Elon Musk are quite popular on YouTube, with a lot of people watching from various walks of life. Painting the picture of a genius entreprenuer, he imbibes the role of a celebrity figure. His videos are often watched for inspiration, as food for thought and sometimes only for his supposed eccentricity. This has led to Elon having a lot of followers, including Youtubers that hold discussions about him on their own channels. Due to this and his name being used abundantly in plenty of content online, one might find it hard to filter and catch hold of new videos that actually have him in it as opposed to having some other Youtubers discussing him. All we try to do is make his videos prompt and more accessible.\n",
    "\n",
    "## Solution proposal\n",
    "The aim is to seach YouTube daily for videos with Elon in them and save the links to those videos in a file. Those videos are to be checked that appear in results when searched for 'Elon Musk'. The results are filtered to show only those videos which have been published in the last 24 hours (Since the script is meant once every day).\n",
    "\n",
    "A face detection package is used and in order to recognise the face as Elon's, a machine learning model is made use of. The model is trained using faces of Elon. Still images from the videos to be checked for Elon are captured. The faces are isolated and a prediction is made using the trained model. If the number Elon Musk faces in the (multiple) images of a video are predicted to be more than a predefined threshold, then that video is flagged to have Elon in it. Finally the list of links are saved in a file on the desktop.\n",
    "\n",
    "### Other possible solutions\n",
    "(.) The face detection package used can also recognise faces, but instead I chose to build a machine learning model myself.\n",
    "(.) Instead of only taking the still images from the videos, the whole video could be made use of for classification into categories: With Elon and Without Elon. But that seems to be bandwidth heavy and alon harder (read longer) to train. I think the solution can be less hungry for data.\n",
    "(.) Audio could be made use of in training instead. But again the entire audio length will have to be made use of.\n",
    "(.) One might think that the title of the video or it's description or channel name can be used to tell if the video contains Elon. But since videos with Elon are released by multiple sources, the channels can't be kept track of. Some of the videos do not even have a description, hence using the video metadata for classification might not turn out to be such a great idea.\n",
    "\n",
    "## Methodology\n",
    "### Elon Musk videos are searched for on YouTube\n",
    " Selenium is used to go to each video link. The links are to be manually collected initially, for training purposes. Selenium then takes screenshots in regular intervals. It achieves this by seeking through the video. Once a particular video is done, it moves on to the next video and repeats until all videos have had their still images collected. \n",
    "### Faces are extracted and used for training\n",
    " As soon as there is a screenshot taken, all the faces in the image are extracted using the 'face_recognition' package and saved in the 'save_path' location provided. The faces are then manually sorted into two classes. One that is Elon's face and another which is not Elon's face. These images are then used to train a CNN model. The trained model is then saved and used to classify new videos.\n",
    "### New videos are checked for Elon\n",
    " 'Elon Musk' is searched for on YouTube. The results are filtered to show only videos from the past 24 hours and then the first 10 videos videos have their still images taken at regular intervals. If a video has more than a predefined percentage of images that test positive for Elon, then that link is flagged and appended to be saved in a file after all the other videos have been checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:46:12.224733Z",
     "start_time": "2020-03-26T00:46:12.203779Z"
    },
    "code_folding": [
     25
    ]
   },
   "outputs": [],
   "source": [
    "# Takes YouTube links or a search term as parameter and saves list of videos with Elon. \n",
    "# Either 'links' or 'search_term' must be provided for it to work. If 'delete_images' is\n",
    "# set to true then the images are deleted after analysis.\n",
    "def get_faces(links = None, save_path = None, search_term = None, delete_images = False): \n",
    "    \n",
    "    #Selenium to save the still images of each video.\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver import ActionChains\n",
    "    \n",
    "    #To load the trained model for prediction and for image preprocessing.\n",
    "    from tensorflow.keras.preprocessing import image as tf_image\n",
    "    from tensorflow.keras.models import load_model\n",
    "    \n",
    "    #For recognising faces, saving and navigating directories.\n",
    "    import matplotlib.pyplot as plt\n",
    "    import face_recognition\n",
    "    import os\n",
    "    import shutil\n",
    "    import time\n",
    "    import csv\n",
    "    import numpy as np\n",
    "    \n",
    "    image_classifier = load_model(r\"C:\\Users\\Anurag\\Desktop\\super_model\")\n",
    "    \n",
    "    # Initialising filename for each face image, full image(debugging purposes)\n",
    "    # and the list to append links with elon.\n",
    "    image_save_number = 1\n",
    "    main_image_save_number = 1   \n",
    "    links_with_elon = [] \n",
    "    \n",
    "    #Takes the first ten links in the results page for the given 'search_term'.   \n",
    "    if search_term:\n",
    "        \n",
    "        options = Options()\n",
    "        path_to_extension = r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\3.7_0\"\n",
    "        #options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"load-extension=\" + path_to_extension)\n",
    "        options.add_argument(\"start-maximized\")\n",
    "        options.add_argument(\"disable-infobars\")\n",
    "        options.add_argument(\"--mute-audio\")\n",
    "        driver=webdriver.Chrome(options=options, executable_path=r\"C:\\Users\\Anurag\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "        \n",
    "        driver.get(\"https://www.youtube.com/\")\n",
    "        time.sleep(3)\n",
    "        WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input#search\"))).send_keys(search_term)\n",
    "        driver.find_element_by_css_selector(\"button.style-scope.ytd-searchbox#search-icon-legacy\").click()\n",
    "        time.sleep(3)\n",
    "        #Filter results to show only the videos uploaded in the last 24 hours.\n",
    "        driver.find_element_by_xpath(\"/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[1]/div[2]/ytd-search-sub-menu-renderer/div[1]/div/ytd-toggle-button-renderer/a/paper-button\").click()\n",
    "        driver.find_element_by_xpath(\"/html/body/ytd-app/div/ytd-page-manager/ytd-search/div[1]/ytd-two-column-search-results-renderer/div/ytd-section-list-renderer/div[1]/div[2]/ytd-search-sub-menu-renderer/div[1]/iron-collapse/div/ytd-search-filter-group-renderer[1]/ytd-search-filter-renderer[2]/a/div\").click()\n",
    "        time.sleep(5)\n",
    "        #Find and save the first ten links.\n",
    "        links = [my_href.get_attribute(\"href\") for my_href in WebDriverWait(driver, 5).until(EC.visibility_of_all_elements_located((By.CSS_SELECTOR, \"a.yt-simple-endpoint.style-scope.ytd-video-renderer#video-title\")))][:10]\n",
    "        driver.quit()\n",
    "        \n",
    "    #Loading adblocker extension on to Chrome.\n",
    "    chrome_options = Options()\n",
    "    path_to_extension = r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\3.7_0\"\n",
    "    chrome_options.add_argument(\"load-extension=\" + path_to_extension)\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "    #Since headless chrome can't go fullscreen, photo quality is affected. Do not use.\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"--window-size=1200,1100\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--mute-audio\")\n",
    "    #Instance variable for Chrome browser.\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\Anurag\\Downloads\\chromedriver_win32\\chromedriver.exe\",options=chrome_options)\n",
    "            \n",
    "    def visit_links_and_get_faces(link):\n",
    "        nonlocal image_save_number\n",
    "        nonlocal main_image_save_number\n",
    "        nonlocal links_with_elon \n",
    "        \n",
    "        def seek(url_time):\n",
    "            #Seeking by appending time to the URL of the video.\n",
    "            url_with_time = \"https://youtu.be/\" + (link.split(\"watch?v=\"))[1] + \"?t=\" + str(url_time)\n",
    "            driver.get(url_with_time)\n",
    "            time.sleep(2)\n",
    "            #To fullscreen and get a higher quality image.\n",
    "            driver.find_element_by_css_selector(\"#movie_player > div.ytp-chrome-bottom > div.ytp-chrome-controls > div.ytp-right-controls > button.ytp-fullscreen-button.ytp-button\").click()\n",
    "#             element = driver.find_element_by_css_selector(\"#movie_player > div.html5-video-container > video\")\n",
    "#             driver.execute_script(\"arguments[0].click();\", element)\n",
    "            \n",
    "        def get_video_screeshot_timings():\n",
    "            # Get the length of the video in order to determine \n",
    "            # how many images need to be collected.\n",
    "            video_length = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"\"\"#movie_player > div.ytp-chrome-bottom > div.ytp-chrome-controls > div.ytp-left-controls > div > span.ytp-time-duration\"\"\"))).text\n",
    "            hh_mm_ss = [int(i) for i in video_length.split(':')]\n",
    "            # If the video is less than an hour long, append dummy zeros\n",
    "            # in order to have uniform length for \"hh_mm_ss\".\n",
    "            if len(hh_mm_ss) < 3:    \n",
    "                hh_mm_ss.insert(0,0) \n",
    "            length_in_seconds = (hh_mm_ss[0]*3600) + (hh_mm_ss[1]*60) + (hh_mm_ss[2])\n",
    "            no_of_screenshots =  length_in_seconds//60 #A screenshot every minute (60s)\n",
    "\n",
    "            if no_of_screenshots == 0: #In case of short videos (<=2mins) take. \n",
    "                no_of_screenshots = 4  # only 4 screenshots per video.\n",
    "            elif no_of_screenshots > 30: #Take 30 screenshots for long videos.\n",
    "                no_of_screenshots = 30\n",
    "            #'url_time_period' is the interval between each screenshot for a particular video.    \n",
    "            url_time_period = length_in_seconds//no_of_screenshots \n",
    "            return length_in_seconds, url_time_period\n",
    "\n",
    "        #Create the 'save_path' directory.\n",
    "        os.makedirs(save_path, exist_ok = True)\n",
    "\n",
    "        #Go to YouTube page with video.\n",
    "        driver.get(link) \n",
    "        \n",
    "        #Get time interval for this video between every screenshot.\n",
    "        length_in_seconds, url_time_period = get_video_screeshot_timings()\n",
    "        \n",
    "        #Take a screenshot once every 'url_time_period' interval and save the faces.  \n",
    "        for url_time in range(1, length_in_seconds, url_time_period):\n",
    "            seek(url_time)\n",
    "            image_address = save_path + \"\\\\temporary.png\"\n",
    "            driver.save_screenshot(image_address)   \n",
    "            \n",
    "            #Load the image onto the face recognition package and find coordinates of faces.\n",
    "            image = face_recognition.load_image_file(image_address)\n",
    "            face_locations = face_recognition.face_locations(image)\n",
    "             \n",
    "            #\"link[-5:]\" in \"face_save_path\" used to group all photos from same video later.\n",
    "            face_save_path = save_path + \"\\\\\" + link[-5:] + \"_\" + str(image_save_number) + \".png\"\n",
    "            #The main image is saved only for debugging purposes.\n",
    "            image_save_path = save_path + \"\\\\\" + str(main_image_save_number) + \"OGG\" + \".png\"\n",
    "            main_image_save_number += 1\n",
    "            plt.imsave(image_save_path, image)\n",
    "            #One coordinate for each corner of the face square.\n",
    "            for coordinates in face_locations:\n",
    "                plt.imsave(face_save_path, image[coordinates[0]:coordinates[2],coordinates[3]:coordinates[1]])\n",
    "                image_save_number += 1\n",
    "                \n",
    "            os.remove(image_address)\n",
    "        #-----------------------------PREDICTION------------------------#\n",
    "        # Prediction is only made for the videos when 'search_term' is provided. \n",
    "        # If 'search_term' is not provided, but 'links' is, then it means that \n",
    "        # images need to be downloaded for training and there is no reason to \n",
    "        # make prediction in that case\n",
    "        if search_term:\n",
    "            images_by_video = []\n",
    "            \n",
    "            # Going through all the images for current video\n",
    "            # in 'save_path' and  preprocessing them.\n",
    "            for root, dirs, files in os.walk(save_path):\n",
    "                for name in files:\n",
    "                    if name[:5] in link:\n",
    "                        img = tf_image.load_img(os.path.join(root,name), target_size = (64, 64))\n",
    "                        img = tf_image.img_to_array(img)\n",
    "                        img = np.expand_dims(img, axis = 0)\n",
    "                        images_by_video.append(img)\n",
    "\n",
    "            results = []   #To save the predictions for all images from current video.\n",
    "            \n",
    "            #Making predictions and appending the result.\n",
    "            for single_image in images_by_video:\n",
    "                result = (image_classifier.predict(single_image))[0][0]\n",
    "                results.append(result)\n",
    "    \n",
    "            if len(results) == 0:  #Go to next link if no faces are detected.\n",
    "                return\n",
    "\n",
    "            # If Elon is present the result is 0 else it is 1. So if there are more than \n",
    "            # 60% of images that test true for Elon then append the current to the final list.\n",
    "            elon_q = sum(results)/len(results)\n",
    "            threshold = 0.4  #0.4 is the complement of 60%.\n",
    "\n",
    "            if elon_q < threshold:\n",
    "                links_with_elon.append(link)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    for link in links:\n",
    "        try:\n",
    "            # Sometimes live videos are parsed and this causes an error. \n",
    "            # Some other times, due to Selenium there can be 'timeout' errors.\n",
    "            # Overall errors are limited to only those two situations.\n",
    "            visit_links_and_get_faces(link)\n",
    "        except Exception as e: print(e)\n",
    "    \n",
    "    if delete_images:\n",
    "        shutil.rmtree(save_path)\n",
    "    # Attempt to save the list only when 'search_term' is provided and\n",
    "    # not when 'links' is provided.\n",
    "    if search_term:\n",
    "        locationss = r\"C:\\Users\\Anurag\\Desktop\\linksWithElon.csv\"\n",
    "        with open(locationss,\"w\") as f:\n",
    "            wr = csv.writer(f,delimiter=\"\\n\")\n",
    "            wr.writerow(links_with_elon)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs 24 hours after the last time it terminated.\n",
    "import time\n",
    "while True:\n",
    "    get_faces(save_path=r\"C:\\Users\\Anurag\\Desktop\\Oorsule\",search_term=\"elon musk\")\n",
    "    time.sleep(60*60*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:35:12.780402Z",
     "start_time": "2020-03-26T00:35:12.775443Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import time\n",
    "import numpy \n",
    "\n",
    "# Normalizing the images and image augmentation by randomly zooming and randomly flipping  \n",
    "# some of the training images.This creates variation in the training data which might help\n",
    "# it generalise better.\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   zoom_range = 0.2,horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_path = r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Test\"\n",
    "train_path = r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of images with Elon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T01:26:44.864222Z",
     "start_time": "2020-03-26T01:26:44.859259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\\Melon\\179.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T01:27:06.209250Z",
     "start_time": "2020-03-26T01:27:06.204263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCAAkACQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD8q8BZCjqSu8Y59K7z9ma58MaX8UrLWvFGWitWaSCJlypkzwW9hmuDRRLtLyr948DqK9R/ZC+FC/Fj4inTdQIS1s4lnuJG6Abv54Br+9uLsY8Bw3ia17Wg9fkfmeQYeWJzelCKu7n1bZ+MPBs2uRpqOtWlnLdH91HPcKhb6ZPNeuaJqH/CP2MZa9QRyr8h3DH4Gvnj4+fDGxm8XeHr/wAJWFoqLA8c6TQiRZUUgKDnlSfUV2ng/wAGeOILSOOSF47Xyw0cJuN6Rj0Gen0r/NzOXQxtSVdy3Z/SWGp1sOlTY74qa34pufFjy6ND9ohMK/P1wcnIoqS6uRazGGZgWHWivJhUlGKSNnRi3e5+eMLfPlCN20sGIr2f9kPxV4rsLrVtG8K2sDNcQRSTtKv8IJHOOcc9q8XidyykxPjymGSO2TXYfBaS7h8UxWlpPNDLJGhSWGUowI6civ8ASXxDwksdwbi6S19xv7j+ceFsT9VzulPbU+wraz1S8v0u9ZhEW0HaiNuCfStpfGWtx2psLe+YRgEKMVh+EZ/E+l28EXi9jcpNHhJTF82B3yODVvXZdPsrj7SsTLEB9Pxr/OPGRtN0+zP6So4iM4czIpormeQyzy/MxyaK53UviFYw3bRwXA2j/aFFJUXYTrQufF0iCOFACeYmzk+9aHhfV7/R9Xgv7GbbIjqBkZGKKK/1AzqMZZbVTV1yv8j+XME2sTG3dH0p4a+KfjG50iKOa/UrjhdpwP1ql4p+IXii9/0Ke+Gw+i80UV/njxFRpQzasoxS1fQ/esHObw0deh0fhTwjol9oyXd3bmSRySzsck0UUV8tJu56kW7H/9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename=r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\\Melon\\243.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T01:27:37.184004Z",
     "start_time": "2020-03-26T01:27:37.178992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCAAkACQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD4t/Zs8MWmj+NPEvifWrlvOi8QX0D+aMNlbhwS3vX0x4G+KPw2l8RwaDN4jSGeQ7YvMhcIx9A2MV4t8OvCes2ni/W9E8Txt/aF5ePez+aBmRpPmZmA7k5J969K0H4afFe9MarrlnDpSqxlt105WfP8JVz0wOtfL4marzufp+WQlhcNGNj3/V7bTX0Qzvrduke04Jb0HWvln4sfFLwhdatcaVHa37wqWQahJZsLcuDjG4/zxivQfEl5cvoemeFm1aVUTUWhmuerSAjKg/lXnPxA8C/Ei41e4stV1W0uNKkl220UVt5ZSHaMbueWzmoo00p3OnF1XKNkj5S+JHhjwqnjG8n1o3BedxJF5KgjYRx175Bor2G5tvBGn3Utprvh+C6nSQjzJY2YhRwBx9P1or73D4mjGhFN9EfFVMurTm5Lqe8/tGaLofhT9rvxVc+CNTF3ol3qE0+iXSPlZLV5WaPHpgHbg9MV6J4X19U8HsYZf3rxlfmPfFQ/8FDPgpL8FdQ8LfEpJbRLG/vH0t5YZBhpiPMTA9wrCuKvtX1fW/AMsHgq4FvqKx87os+4IHfNfD4yjKhiHHofW5XiY1cMr7ow/EsXxFltLfw6fCSuW1oXcmqR3X7tYQOF553fhiuk+P8A4htLPwnb3dhhJpAse3POcda4LUL7xg+iW9j4c8S67/asYP21bzDRsT/skYA9qz/iv4nbQfAdjb+Ir37VqIhMtxLswkYGR/P+VZK7nFI6sROMKTbNz4F/sufE39onwve+M/CF/FHbWeqvYSAtHkyJHHIx+Yg/8tRRXxjpn7Xfi3wPqmr2egajqEdreao1yi2sxVT+7jjzgd/3dFfoNGl+6jp0XQ+HlmrUnY/QH/gul8cvGHxE8D/B/wAP6vYaXa2//CcPNINMsRAZW8hlG7aecBjj3rwr4BfFPxjbeNpfC098lzbWV0LWFrlMuYx0yQRkj1oor5XNUnUR3ZHKXNLU+j/Fd66aVNOsMe5I9wOzvivg74x+OvE3xJ+KjeDvEWpMunsZA8Nr8m5Y8lVPXjPWiiuLBpe1PTzSUvZrU8xaKwsf3CaTayAE4aWHcepooor9Bpt+zR+aVG1UZ//Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename=r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\\Melon\\117.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of images without Elon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T01:29:28.680397Z",
     "start_time": "2020-03-26T01:29:28.676380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCAAkACQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD82f8Agh14K0jxZ+2bdavfx7rjw94LvdQ01s42zPNbWhP4xXUo/Gv2s8aeJPENx4EgsrGyt4ooBudnYA5x169ea/Hb/giB4RvtH+MmpfF3SvGGkfaW0bUNLPhq4Sf7TPEgtJ/PVljMYHnGBQrMGYCUgfJg/SPxW/al/bC8bfEO68MaJcaxLb2iN9n06+8AxaTaXQ3cGJprjz2JwANyAlSCFr5vP4Sr5jo17sV+p9jw7U9hg05RfvN/Poe6eNo9YmgkuryZFYncuG+ZhXEyXupadepJqm5crmNiDyP5Vwv7TfhP9o/wv4ovPDXg/wAYzTtpKot7FY3CQSysoIwrujjBI2ndgZxz1qv8JtR+JWvaUJvHNnf2wiiH2iLUNThuyDgE4eOOMAgkgjaR6E12cPYiWEr8ynZnn8YYWOOw/I6d0fCn/BTrwvdaN+1jqWu3UQSPX9Jsr+224+ZFi+zE/Xdbv/OivqX9pD9kr4U/tKeN7Txl4n8dalY3OnaUmmrBZ3Fuq7ElllBIkGc/vj7Yx70V9di8dgp4mcpVEm3rv/kfGZTlmZUstpQVJ2UUltstF17Hzt/wSS+JkvgP9qu28NeeQniSxe3t0+QD7VF++iJLEHlUmjCg/M0qjBOK/UjVPj94JsvizpvhzwD8E4GuWWS+8YeIdO06G41IwwxPKyI0gJxI6JHtzuAb5PmCivwm8N67rnhPX7PxR4b1S4sdR066jubC9tZTHLbzRsHSRGHKsrAEEdCK/W7wrD41+InhfR/iX4Z/tbwzqfiXw5bX9zY6P5Cz3ckyEPH5txFIPLR0KhgvJVsgcgfJ57gV7eOIto9H69D7bhnMPceFb13V/wAl/XU0tY/aAt/il8RdU8XfD8XPnPI73DajpzpAVOW2MxHytnA4yQSeOMVY8T+NZvEWhhbHTVsGI23EKqBtbuOOD9e9eYv8LP2h9J8QtP4u1LVmtyF8+JtS05yBjgEWsEZBxgc8+2a259ZutLtl0WfexHERkbLD/eJ61yUIxdePKe1mnPTwUvaabv7y5cH4T6ZDb/8ACwvFHhW1vbi3E0UWsXdvHL5RJAOJOcZDe2c0V6l4E/ZH8MftFeHYfiKfgN8OdagWOKwtdQ8ZacGu3W3iSJgJQq+dGJFkCsdxGDHn93tUr7Onwb9dgq6+1qfjVfxWweV1XhKitKGj1XQ/Hj9nH4daH8W/2gvA3wq8SXN1Dp3ibxfpulX81k6rNHDcXUcLtGWVlDhXJBKsM4yD0r9X/GXj3UfE/hrUvEVlptrosngO48NeGdBtdHMqW8emT6A98lv5UjukSwzRytGIRHk3lw0vmuyOhRXmZv8A7vb1/Q+nyhtZlBnkzfEPxlqzy6xfa9O8zyFmJfgnNa/wK0ZPiv8AFrRfBvia/uUt9V1q2sp5rVlWVI5ZAjFSwYBsHjII9jRRXj4CMfrEFbqvzPp84qVJYGpdvSMvyPFr39un40SeLPEGnC10hLHTdcmsdH06KO4SDTrSIKiQRKs4wvDOWYs7ySSO7Mzk0UUV+jyq1YyaUn95+N0MPQlRi3BNtLouyP/Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename=r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\\Nelon\\6.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T01:29:40.076019Z",
     "start_time": "2020-03-26T01:29:40.071056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename=r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\\Nelon\\45.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T01:29:56.985210Z",
     "start_time": "2020-03-26T01:29:56.981210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCAAkACQDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD84v2X9W8V+JPjRJrnim4ub+5ZR9oubhixIyeefev0M+GfiHwzpcdvHrj2qQhNoMj4JNfCf7MXhPVXnu9Tsbc77oGEPn7mOc16/Z/DLxDN4SudQ8V312l7Zy5tlEjAHjPNcmKpqpi5XPqsurOlgo8p9nXHinwBBEbiLWItqDI+cYrz3xh8ffhCX/sqx8VW0l2xbIaTlSO1fPviXRvH8nwPTxFZwTySS3qQyeVIxPlncCf0rx+b4PeJdBudT8cSX0sFvHYs0cE7HeX2nJ5561wPDxc9zqxWLmqd0jzP9rH9pL4oXnxv1U+HfEctvaRERxpERtO0kZor578a69q+q+KL28vgHkadgWMp5GaK640IW3Pm5Yyq5Nn378MfiR4E+H/xX/sDwXuk0K5lAD3SkE89efoK9u/aD+LXwn8F6B5NzrSxzTxb2WN95B+gr4e1XXtN84TW/iSzLI26NhcqSD+dXIfFvhjxI1xc+NdVF5cSL+72XAJ/LNexi8LGc+ZM5stx/JT5ZH1toP7X/wAG7bwBb+C7O8SSVIml3TfKNwPHB+teLfthftL6evwtey0Oe2a9uuE8rGdh69PavCfHt94RurO4fSbCW1nWIiKR3ByMe1eYapd3mpKo1S4aUxjEeTwBXlvAyUrtnqYjM6cqdkYMVtZ3QN1qCkyysWbn1oq6VhBw0f0orVYfTc8R4iTd7HYydqfpyiTUY1YnHPT6GiiuuPxL1I6MNfd7lSkrEhBxzXOXigA0UVOIbSCGpRZRnkUUUVyx2NbK5//Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(filename=r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\\Nelon\\20.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import time\n",
    "import numpy \n",
    "\n",
    "# Normalizing the images and image augmentation by randomly zooming and randomly flipping  \n",
    "# some of the training images.This creates variation in the training data which might help\n",
    "# it generalise better. There are 174 of other's faces and 99 of Elon's faces.\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   zoom_range = 0.2,horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_path = r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Test\"\n",
    "train_path = r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Seperate_faces\\To feed tensorflow\\Train\"\n",
    "\n",
    "# Assuming that training a model will be easy, we can try to brute force search the \n",
    "# hyperparameter space for the ones that have the highest accuracy or lowest loss.\n",
    "# The various combinations are listed below. These values will be used in tuning\n",
    "# multiple hyperparameters throught all the layers.This will result in a total of\n",
    "# 1152 models.\n",
    "batch_size_list = [16,32,64]\n",
    "kernel_size_list = [2,3,4,6]\n",
    "extra_layers = [True,False]\n",
    "\n",
    "for batch_size_var in batch_size_list:\n",
    "    for filter_var in batch_size_list:\n",
    "        for kernel_var in kernel_size_list:\n",
    "            for pool_var in kernel_size_list:\n",
    "                for first_extra_layer in extra_layers:\n",
    "                    for second_extra_layer in extra_layers:\n",
    "                        for third_extra_layer in extra_layers:\n",
    "                            # Values 16,32,64 are tried to get the number of training examples \n",
    "                            # that go through the network in one pass before updating the\n",
    "                            # weights. This is the 'batch_size' argument here\n",
    "                            training_set = train_datagen.flow_from_directory(train_path,target_size = (64, 64),batch_size = batch_size_var, class_mode = 'binary')\n",
    "                            test_set = test_datagen.flow_from_directory(test_path,target_size = (64, 64),batch_size = batch_size_var, class_mode = 'binary')\n",
    "                            \n",
    "# Creating a sequential CNN with two pairs of alternating convolution layers and maxpooling \n",
    "# layers. The number of filters and kernel sizes are varied of the convolutional layer.\n",
    "# The pool size is varied in the maxpooling layer. Also three other layers are added in all \n",
    "# combinations. \n",
    "# ReLU activation functions are in all layers but the last. The last layer makes use of the \n",
    "# sigmoid activation function to output a 0 or 1, since this is a case of binary classification.\n",
    "# Since the images are already zoomed in on the face, the images are ensured to not lose size\n",
    "# by keeping padding='same' throughout the layers.\n",
    "# A binary crossentropy loss function is minimized by the Adam optimizer.\n",
    "# The accuracy and loss for both the training and test set is logged and can be viewed in \n",
    "# TensorBoard.\n",
    "                            classifier_cnn = Sequential()\n",
    "                            classifier_cnn.add(Conv2D(filter_var, (kernel_var, kernel_var), input_shape = (64, 64, 3), activation = 'relu', padding=\"same\"))\n",
    "                            classifier_cnn.add(MaxPooling2D(pool_size = (pool_var,pool_var), padding=\"same\"))\n",
    "                            classifier_cnn.add(Conv2D(filter_var, (kernel_var, kernel_var), activation = 'relu', padding=\"same\"))\n",
    "                            classifier_cnn.add(MaxPooling2D(pool_size = (pool_var,pool_var), padding=\"same\"))\n",
    "                            if first_extra_layer:\n",
    "                                classifier_cnn.add(Conv2D(filter_var, (kernel_var, kernel_var), activation = 'relu', padding=\"same\"))\n",
    "                                classifier_cnn.add(MaxPooling2D(pool_size = (pool_var,pool_var), padding=\"same\"))\n",
    "                            if second_extra_layer:\n",
    "                                classifier_cnn.add(Conv2D(filter_var, (kernel_var, kernel_var), activation = 'relu', padding=\"same\"))\n",
    "                                classifier_cnn.add(MaxPooling2D(pool_size = (pool_var,pool_var), padding=\"same\"))\n",
    "                            classifier_cnn.add(Flatten())\n",
    "                            if third_extra_layer:\n",
    "                                classifier_cnn.add(Dense(units = 128, activation = 'relu'))\n",
    "                            classifier_cnn.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "                            classifier_cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Callback functions are created to be called during the fit method in order to log the accuracy\n",
    "# and loss at each epoch for each model.\n",
    "                            NAME = \"bsv_\" + str(batch_size_var) + \"-fv_\" + str(filter_var) + \"-kv_\" + str(kernel_var) + \"-pv_\" + str(pool_var) + \"-fel_\" + str(first_extra_layer) + \"-sel_\" + str(second_extra_layer) + \"-tel_\" + str(third_extra_layer)\n",
    "                            tensorboard = TensorBoard(log_dir = 'logs\\{}'.format(NAME))\n",
    "                            \n",
    "                            call_back = classifier_cnn.fit(training_set, epochs = 50, validation_data = test_set, callbacks=[tensorboard])\n",
    "                            history_address = r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Nbs\\history_logs\\\\\" + NAME\n",
    "                            numpy.save(history_address, call_back.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:12:18.820223Z",
     "start_time": "2020-03-26T00:12:14.592319Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read log history and save only the required values seperately\n",
    "\n",
    "import os\n",
    "import numpy\n",
    "loss_train = []\n",
    "acc_train = []\n",
    "loss_val = []\n",
    "acc_val = []\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(r\"C:\\Users\\Anurag\\1First\\Projects\\EM files\\Nbs\\history_logs\"):\n",
    "    for name in files:\n",
    "        file_address = os.path.join(root,name)\n",
    "        history = numpy.load(file_address,allow_pickle='TRUE').item()\n",
    "#Only the four required metrics are accessed\n",
    "        history['loss'].insert(0,name)\n",
    "        history['accuracy'].insert(0,name)\n",
    "        history['val_loss'].insert(0,name)\n",
    "        history['val_accuracy'].insert(0,name)\n",
    "        \n",
    "        loss_train.append(history['loss'])\n",
    "        acc_train.append(history['accuracy'])\n",
    "        loss_val.append(history['val_loss'])\n",
    "        acc_val.append(history['val_accuracy'])\n",
    "#Metrics are saved seperately\n",
    "# numpy.savetxt(r\"C:\\Users\\Anurag\\Desktop\\tableau metricss\\loss_train.csv\",numpy.transpose(loss_train),fmt=\"%s\",delimiter=\",\")\n",
    "# numpy.savetxt(r\"C:\\Users\\Anurag\\Desktop\\tableau metricss\\loss_val.csv\",numpy.transpose(loss_val),fmt=\"%s\",delimiter=\",\")\n",
    "# numpy.savetxt(r\"C:\\Users\\Anurag\\Desktop\\tableau metricss\\acc_train.csv\",numpy.transpose(acc_train),fmt=\"%s\",delimiter=\",\")\n",
    "# numpy.savetxt(r\"C:\\Users\\Anurag\\Desktop\\tableau metricss\\acc_val.csv\",numpy.transpose(acc_val),fmt=\"%s\",delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:12:21.721660Z",
     "start_time": "2020-03-26T00:12:21.702683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find top ten models with highest validation accuracy and top ten models with lowest validation\n",
    "# loss seperately. The models are ranked on the basis of the average of their values in the last\n",
    "# 'average_of' epochs. Here the average of the last 5 are used to rank the models.\n",
    "\n",
    "average_of = 5\n",
    "\n",
    "loss_val_rank = []\n",
    "for i in loss_val:\n",
    "    avg = sum(i[-average_of:])/average_of\n",
    "    loss_val_rank.append([i[0],avg])\n",
    "loss_val_rank_np = numpy.array(loss_val_rank)\n",
    "loss_val_rank_np = loss_val_rank_np[loss_val_rank_np[:,1].argsort()]\n",
    "\n",
    "acc_val_rank = []\n",
    "for j in acc_val:\n",
    "    avg = sum(j[-average_of:])/average_of\n",
    "    acc_val_rank.append([j[0],avg])\n",
    "acc_val_rank_np = numpy.array(acc_val_rank)\n",
    "acc_val_rank_np = acc_val_rank_np[acc_val_rank_np[:,1].argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ranked lists have the same model at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:12:29.767647Z",
     "start_time": "2020-03-26T00:12:29.759695Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['bsv_32-fv_32-kv_4-pv_2-fel_True-sel_True-tel_True.npy',\n",
       "        '0.19210668593645094'],\n",
       "       ['bsv_16-fv_64-kv_3-pv_3-fel_True-sel_True-tel_True.npy',\n",
       "        '0.20732003815472128'],\n",
       "       ['bsv_32-fv_64-kv_4-pv_3-fel_True-sel_True-tel_True.npy',\n",
       "        '0.2112583436816931'],\n",
       "       ...,\n",
       "       ['bsv_32-fv_16-kv_2-pv_4-fel_False-sel_True-tel_False.npy',\n",
       "        '0.5823477101325989'],\n",
       "       ['bsv_32-fv_16-kv_2-pv_4-fel_True-sel_False-tel_False.npy',\n",
       "        '0.5924702906608582'],\n",
       "       ['bsv_64-fv_16-kv_2-pv_4-fel_True-sel_True-tel_False.npy',\n",
       "        '0.6141326824824016']], dtype='<U56')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_val_rank_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:12:35.870007Z",
     "start_time": "2020-03-26T00:12:35.865973Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['bsv_32-fv_32-kv_4-pv_2-fel_True-sel_True-tel_True.npy',\n",
       "        '0.9380645155906677'],\n",
       "       ['bsv_16-fv_64-kv_6-pv_3-fel_True-sel_False-tel_False.npy',\n",
       "        '0.9264516234397888'],\n",
       "       ['bsv_16-fv_64-kv_6-pv_3-fel_True-sel_False-tel_True.npy',\n",
       "        '0.9264516115188599'],\n",
       "       ...,\n",
       "       ['bsv_32-fv_16-kv_2-pv_4-fel_True-sel_True-tel_False.npy',\n",
       "        '0.6425806522369385'],\n",
       "       ['bsv_32-fv_16-kv_2-pv_4-fel_False-sel_True-tel_False.npy',\n",
       "        '0.629677414894104'],\n",
       "       ['bsv_64-fv_16-kv_2-pv_4-fel_True-sel_False-tel_False.npy',\n",
       "        '0.6090322732925415']], dtype='<U56')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_val_rank_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is used to train and to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:38:03.417041Z",
     "start_time": "2020-03-26T00:37:52.756599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 273 images belonging to 2 classes.\n",
      "Found 113 images belonging to 2 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 9 steps, validate for 4 steps\n",
      "Epoch 1/25\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 0.6854 - accuracy: 0.5971 - val_loss: 0.6399 - val_accuracy: 0.6637\n",
      "Epoch 2/25\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.6495 - accuracy: 0.6374 - val_loss: 0.6353 - val_accuracy: 0.6637\n",
      "Epoch 3/25\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.6311 - accuracy: 0.6374 - val_loss: 0.5999 - val_accuracy: 0.6372\n",
      "Epoch 4/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.6000 - accuracy: 0.6447 - val_loss: 0.5596 - val_accuracy: 0.6549\n",
      "Epoch 5/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.5561 - accuracy: 0.6850 - val_loss: 0.5415 - val_accuracy: 0.6991\n",
      "Epoch 6/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.5542 - accuracy: 0.6777 - val_loss: 0.5215 - val_accuracy: 0.7345\n",
      "Epoch 7/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.5639 - accuracy: 0.6960 - val_loss: 0.4994 - val_accuracy: 0.7257\n",
      "Epoch 8/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.4869 - accuracy: 0.7473 - val_loss: 0.4759 - val_accuracy: 0.7345\n",
      "Epoch 9/25\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.4474 - accuracy: 0.7546 - val_loss: 0.3999 - val_accuracy: 0.8407\n",
      "Epoch 10/25\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.4276 - accuracy: 0.7619 - val_loss: 0.4155 - val_accuracy: 0.7965\n",
      "Epoch 11/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.3753 - accuracy: 0.8278 - val_loss: 0.3771 - val_accuracy: 0.8407\n",
      "Epoch 12/25\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3267 - accuracy: 0.8425 - val_loss: 0.3324 - val_accuracy: 0.8496\n",
      "Epoch 13/25\n",
      "9/9 [==============================] - 0s 44ms/step - loss: 0.3608 - accuracy: 0.8205 - val_loss: 0.4889 - val_accuracy: 0.8053\n",
      "Epoch 14/25\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.3220 - accuracy: 0.8755 - val_loss: 0.3654 - val_accuracy: 0.8142\n",
      "Epoch 15/25\n",
      "9/9 [==============================] - 0s 46ms/step - loss: 0.3079 - accuracy: 0.8681 - val_loss: 0.2987 - val_accuracy: 0.8407\n",
      "Epoch 16/25\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 0.2509 - accuracy: 0.8864 - val_loss: 0.4826 - val_accuracy: 0.7876\n",
      "Epoch 17/25\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.3077 - accuracy: 0.8645 - val_loss: 0.3263 - val_accuracy: 0.7965\n",
      "Epoch 18/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.3045 - accuracy: 0.8498 - val_loss: 0.2648 - val_accuracy: 0.8938\n",
      "Epoch 19/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1947 - accuracy: 0.9267 - val_loss: 0.2723 - val_accuracy: 0.9027\n",
      "Epoch 20/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.2037 - accuracy: 0.9194 - val_loss: 0.2982 - val_accuracy: 0.8673\n",
      "Epoch 21/25\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1617 - accuracy: 0.9487 - val_loss: 0.2837 - val_accuracy: 0.8761\n",
      "Epoch 22/25\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 0.1372 - accuracy: 0.9377 - val_loss: 0.2944 - val_accuracy: 0.8938\n",
      "Epoch 23/25\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1616 - accuracy: 0.9377 - val_loss: 0.3479 - val_accuracy: 0.8584\n",
      "Epoch 24/25\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 0.1108 - accuracy: 0.9707 - val_loss: 0.3727 - val_accuracy: 0.8496\n",
      "Epoch 25/25\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 0.1660 - accuracy: 0.9267 - val_loss: 0.2934 - val_accuracy: 0.9027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2623cf48ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(train_path,target_size = (64, 64),batch_size = 32, class_mode = 'binary')\n",
    "test_set = test_datagen.flow_from_directory(test_path,target_size = (64, 64),batch_size = 32, class_mode = 'binary')\n",
    "\n",
    "classifier_cnn = Sequential()\n",
    "classifier_cnn.add(Conv2D(32, (4, 4), input_shape = (64, 64, 3), activation = 'relu', padding=\"same\"))\n",
    "classifier_cnn.add(MaxPooling2D(pool_size = (2,2), padding=\"same\"))\n",
    "classifier_cnn.add(Conv2D(32, (4, 4), activation = 'relu', padding=\"same\"))\n",
    "classifier_cnn.add(MaxPooling2D(pool_size = (2,2), padding=\"same\"))\n",
    "classifier_cnn.add(Conv2D(32, (4, 4), activation = 'relu', padding=\"same\"))\n",
    "classifier_cnn.add(MaxPooling2D(pool_size = (2,2), padding=\"same\"))\n",
    "classifier_cnn.add(Conv2D(32, (4, 4), activation = 'relu', padding=\"same\"))\n",
    "classifier_cnn.add(MaxPooling2D(pool_size = (2,2), padding=\"same\"))\n",
    "classifier_cnn.add(Flatten())\n",
    "classifier_cnn.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier_cnn.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "classifier_cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "NAME = \"final_classifier\"\n",
    "tensorboard = TensorBoard(log_dir = 'logs\\{}'.format(NAME))\n",
    "\n",
    "classifier_cnn.fit(training_set, epochs = 25, validation_data = test_set, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T00:40:49.871895Z",
     "start_time": "2020-03-26T00:40:49.866909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 32)        1568      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 32)        16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 32)          16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 116,609\n",
      "Trainable params: 116,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_cnn.save(r\"C:\\Users\\Anurag\\Desktop\\super_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possible improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(.) Flagging accuracy can be improved by changing flagging algorithm slightly\n",
    "(.) Model can be trained with more images for better generaliasation\n",
    "(.) Live videos can be included in being flagged\n",
    "(.) Ensemble models can be made use of for training\n",
    "(.) CNN parameter can be used to improve face dectection accuracy in the face_recognition package\n",
    "(.) A time scheduler can be used to ensure that the program runs at the same hour of the day everyday\n",
    "(.) The new videos can be notified to the user actively (by mail or message) rather than passively (currently being saved into a file)\n",
    "(.) Chrome can be made to run windowless bug-free with some amount of effort in Selenium\n",
    "(.) The face recognition feature of the face_recognition package can be made use of instead of training our own model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
